# Aircraft Turbofan Engine Predictive Maintenance

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-orange)](https://www.tensorflow.org/)
[![sklearn](https://img.shields.io/badge/scikit--learn-latest-red)](https://scikit-learn.org/)

Predict when aircraft engines will fail using machine learning and deep learning models. This project analyzes NASA's engine sensor data to give advance warning before failure occurs.

---

## Project Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PROJECT PIPELINE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   ğŸ“Š RAW DATA                    ğŸ”§ PREPROCESSING              ğŸ¤– MODELS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   NASA Dataset        â†’     Clean & Transform      â†’     Train Models
   100 Engines              Remove bad sensors           7 Different AI Models
   21 Sensors               Normalize values             ML + Deep Learning
   20,631 Records           Create sequences             
                                                              â†“
                           
   ğŸ“ˆ EVALUATION                  ğŸ† BEST MODEL               ğŸ’¾ DEPLOY
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Compare Results     â†’     Hybrid CNN-LSTM        â†’     Save & Use
   Find best model           5.27 cycles error            Predict failures
   Generate reports          99.15% accuracy              1 day warning
```

---

## Understanding the Data (Simple Explanation)

### What is the Data?

Imagine 100 aircraft engines, each running until it breaks down. As they run, 21 different sensors measure things like:
- **Temperature**: How hot different engine parts get
- **Pressure**: How much force the air has inside
- **Speed**: How fast the fan and core are spinning
- **Fuel Flow**: How much fuel is being used

### Data Structure Visualization

```
Each Engine Record (One Row):
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Engineâ”‚ Time â”‚  3 Settings     â”‚    21 Sensor Readings        â”‚
â”‚  ID  â”‚Cycle â”‚(Operating Modes)â”‚  (Temperature, Pressure...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1      1       [3 numbers]         [21 numbers]
   1      2       [3 numbers]         [21 numbers]
   ...   ...          ...                  ...
   1     192     [3 numbers]         [21 numbers]  â† Engine Fails!

Total: 100 engines Ã— ~200 cycles each = 20,631 measurements
```

### What We Predict: Remaining Useful Life (RUL)

```
Timeline of Engine Life:
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
0 cycles                                    192 cycles
(Brand New)      Time Passes â†’              (Failure)

At any point, RUL = How many cycles left until failure

Example:
- At cycle 100: RUL = 92 cycles left
- At cycle 150: RUL = 42 cycles left  
- At cycle 190: RUL = 2 cycles left â† URGENT MAINTENANCE!
```

### Data Preprocessing (What We Did)

```
Step 1: Remove Useless Sensors
   21 sensors â†’ 7 don't change at all (removed) â†’ 14 useful sensors
   
Step 2: Normalize Values
   Before: Temperature = 500Â°C, Speed = 2500 RPM
   After:  All values scaled to 0-1 range (easier for AI to learn)
   
Step 3: Create Sequences
   Instead of 1 measurement, look at last 50 cycles together
   
   [Cycle 1, Cycle 2, ..., Cycle 50] â†’ Predict RUL
   [Cycle 2, Cycle 3, ..., Cycle 51] â†’ Predict RUL
   (Like showing the AI a 50-frame video instead of 1 photo)
```

### Visual Data Analysis

**Engine Degradation Over Time:**
![Engine Degradation](results/engine_degradation.png)
*Shows how sensor readings change as engines approach failure. Clear degradation patterns visible.*

**Sensor Correlation Heatmap:**
![Sensor Correlations](results/sensor_correlations.png)
*Identifies which sensors are related to each other. Helps understand which sensors are most important.*

---

## Models Overview (Simple Explanation)

### Machine Learning Models (Traditional Approach)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Random Forest                                           â”‚
â”‚     Think: 100 decision trees voting together               â”‚
â”‚     Result: RMSE = 41.37 cycles                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. XGBoost                                                 â”‚
â”‚     Think: Smart sequential tree building                   â”‚
â”‚     Result: RMSE = 42.11 cycles                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. LightGBM                                                â”‚
â”‚     Think: Faster version of XGBoost                        â”‚
â”‚     Result: RMSE = 41.18 cycles (Best ML Model)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Feature Importance Analysis:**
![Feature Importance](results/feature_importance.png)
*Shows which sensors matter most for predictions. Temperature and pressure sensors are key.*

**Machine Learning Predictions:**
![ML Predictions](results/ml_predictions.png)
*Comparison of Random Forest, XGBoost, and LightGBM predictions vs actual RUL values.*

### Deep Learning Models (Advanced Neural Networks)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LSTM (Long Short-Term Memory)                            â”‚
â”‚     Think: Remembers patterns over time                      â”‚
â”‚     Best for: Time-series data like engine degradation       â”‚
â”‚     Result: RMSE = 18.69 cycles                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. 1D CNN (Convolutional Neural Network)                    â”‚
â”‚     Think: Finds patterns in sensor arrays                   â”‚
â”‚     Best for: Detecting local anomalies                      â”‚
â”‚     Result: RMSE = 15.25 cycles                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  6. Bi-LSTM (Bidirectional LSTM)                            â”‚
â”‚     Think: Looks at data forwards AND backwards              â”‚
â”‚     Best for: Understanding full context                     â”‚
â”‚     Result: RMSE = 17.50 cycles                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  7. Hybrid CNN-LSTM â­ WINNER!                               â”‚
â”‚     Think: CNN extracts features + LSTM learns time patterns â”‚
â”‚     Best for: Combining spatial and temporal learning        â”‚
â”‚     Result: RMSE = 5.27 cycles (BEST!)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Training Visualizations

**LSTM Training Progress:**
![LSTM Training History](results/lstm_training_history.png)
![LSTM Predictions](results/lstm_predictions.png)
*Left: Loss decreases over training epochs. Right: Predicted vs Actual RUL comparison.*

**CNN Training Progress:**
![CNN Training History](results/cnn_training_history.png)
![CNN Predictions](results/cnn_predictions.png)
*CNN model learns spatial patterns in sensor data efficiently.*

**Bi-LSTM Training Progress:**
![BiLSTM Training History](results/bilstm_training_history.png)
![BiLSTM Predictions](results/bilstm_predictions.png)
*Bidirectional processing improves context understanding.*

**Hybrid CNN-LSTM Training Progress:**
![Hybrid Training History](results/hybrid_training_history.png)
![Hybrid Predictions](results/hybrid_predictions.png)
*Best model combines CNN feature extraction with LSTM temporal learning. Notice the tight clustering around the diagonal line (perfect predictions).*

### Model Architecture: Hybrid CNN-LSTM (Best Model)

```
INPUT: 50 cycles Ã— 14 sensors
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  CNN Layers     â”‚  â† Extracts patterns from sensors
    â”‚  (Feature       â”‚     "This sensor combo looks bad"
    â”‚   Extraction)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LSTM Layers    â”‚  â† Learns how patterns change over time
    â”‚  (Temporal      â”‚     "It's getting worse each cycle"
    â”‚   Learning)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Dense Layer    â”‚  â† Combines information
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    OUTPUT: RUL Prediction
    (e.g., "42 cycles until failure")
```

---

## Hyperparameter Tuning (Making Models Better)

### What is Hyperparameter Tuning?

Think of it like tuning a recipe:
- Too much salt = bad
- Too little salt = bland
- Just right = perfect!

Similarly, models have "settings" we can adjust:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Parameters We Tuned:                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Number of Trees (n_estimators)                          â”‚
â”‚     Tried: 100, 200, 300, 400, 500                          â”‚
â”‚     Like: How many expert opinions to combine               â”‚
â”‚                                                              â”‚
â”‚  2. Tree Depth (max_depth)                                  â”‚
â”‚     Tried: 5, 7, 10, 12, 15                                 â”‚
â”‚     Like: How many questions each tree can ask              â”‚
â”‚                                                              â”‚
â”‚  3. Learning Rate                                           â”‚
â”‚     Tried: 0.01, 0.05, 0.1, 0.15, 0.2                       â”‚
â”‚     Like: How fast the model learns (slow = careful)        â”‚
â”‚                                                              â”‚
â”‚  4. Feature Sampling (colsample_bytree)                     â”‚
â”‚     Tried: 0.6 to 1.0                                       â”‚
â”‚     Like: What % of sensors to look at each time            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Process:
  Random Search â†’ Try 20 different combinations
                â†’ Pick the best performing one
                â†’ Improves accuracy by 2-5%
```

### Tuning Results

```
Before Tuning â†’ After Tuning â†’ Improvement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
XGBoost:   42.11  â†’  39.88    â†’  5.3% better
LightGBM:  41.18  â†’  39.24    â†’  4.7% better
```

---

## Installation & Setup

### Prerequisites

- Python 3.8 or higher
- 8GB RAM (16GB recommended)
- Internet connection (for dataset download)

### Quick Start

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd aircraft-engine-predictive-maintenance
```

2. **Create virtual environment**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download the dataset**
```bash
python download_dataset.py
```

---

## Usage

### Run Complete Pipeline

Execute scripts in order (each takes 5-45 minutes):

```bash
# Phase 1: Understand the data
python 01_data_exploration.py           # ~2 minutes

# Phase 2: Clean and prepare data
python 02_data_preprocessing.py         # ~3 minutes

# Phase 3: Train traditional ML models
python 03_ml_baseline.py                # ~10 minutes

# Phase 4-7: Train deep learning models
python 04_deep_learning_lstm.py         # ~15 minutes
python 05_cnn_model.py                  # ~10 minutes
python 06_bilstm_model.py               # ~15 minutes
python 07_hybrid_cnn_lstm.py            # ~20 minutes (BEST MODEL)

# Phase 8: Optimize models
python 08_hyperparameter_tuning.py      # ~30 minutes

# Phase 9: Compare all results
python 09_final_comparison.py           # ~5 minutes
```

### Use Trained Model for Predictions

```python
from tensorflow import keras
import joblib
import numpy as np

# Load best model
model = keras.models.load_model('models/hybrid_cnn_lstm.h5')
scaler = joblib.load('models/scaler.pkl')

# Your engine data (50 cycles Ã— 14 sensors)
your_data = np.array([...])  # Shape: (1, 50, 14)

# Predict
predicted_rul = model.predict(your_data)[0][0]
print(f"Remaining Useful Life: {predicted_rul:.0f} cycles")
print(f"Approximately {predicted_rul/5:.1f} days until maintenance needed")
```

---

## Results Summary

### Performance Comparison

| Rank | Model | Error (RMSE) | Accuracy (RÂ²) | Training Time |
|------|-------|--------------|---------------|---------------|
| ğŸ¥‡ | **Hybrid CNN-LSTM** | **5.27** | **99.15%** | ~20 min |
| ğŸ¥ˆ | 1D CNN | 15.25 | 92.90% | ~10 min |
| ğŸ¥‰ | Bi-LSTM | 17.50 | 90.66% | ~15 min |
| 4 | LSTM | 18.69 | 89.35% | ~15 min |
| 5 | LightGBM | 41.18 | 62.89% | ~3 min |
| 6 | Random Forest | 41.37 | 62.54% | ~5 min |
| 7 | XGBoost | 42.11 | 61.18% | ~3 min |

### Visual Performance Comparison

**Comprehensive Model Comparison:**
![Final Comparison](results/FINAL_COMPREHENSIVE_COMPARISON.png)
*Side-by-side comparison of all 7 models across multiple metrics (RMSE, MAE, RÂ²).*

**Performance Improvement Chart:**
![Performance Improvement](results/performance_improvement_chart.png)
*Shows the dramatic improvement from traditional ML to deep learning, with Hybrid CNN-LSTM achieving the best results.*

### Key Insights

- ğŸš€ **87.5% Improvement**: Deep learning beats traditional ML significantly
- â° **Early Warning**: Predicts failure ~5 cycles (1 day) in advance
- ğŸ¯ **High Accuracy**: 99.15% RÂ² score means very reliable predictions
- ğŸ’ª **No Overfitting**: Model generalizes well to new engines

---

## Project Structure

```
aircraft-engine-predictive-maintenance/
â”‚
â”œâ”€â”€ ğŸ“ data/                          # Dataset files
â”‚   â”œâ”€â”€ train_FD001.txt              # Raw training data (100 engines)
â”‚   â”œâ”€â”€ test_FD001.txt               # Raw test data (100 engines)
â”‚   â”œâ”€â”€ RUL_FD001.txt                # Ground truth RUL values
â”‚   â”œâ”€â”€ train_processed.csv          # Cleaned & preprocessed data
â”‚   â””â”€â”€ readme.txt                   # Dataset documentation
â”‚
â”œâ”€â”€ ğŸ“ models/                        # Saved trained models
â”‚   â”œâ”€â”€ hybrid_cnn_lstm.h5           # ğŸ† Best model
â”‚   â”œâ”€â”€ lstm_model.h5                # LSTM model
â”‚   â”œâ”€â”€ cnn_model.h5                 # CNN model
â”‚   â”œâ”€â”€ bilstm_model.h5              # Bi-LSTM model
â”‚   â”œâ”€â”€ random_forest.pkl            # Random Forest
â”‚   â”œâ”€â”€ xgboost.pkl                  # XGBoost
â”‚   â”œâ”€â”€ lightgbm.pkl                 # LightGBM
â”‚   â”œâ”€â”€ scaler.pkl                   # Data normalizer
â”‚   â””â”€â”€ feature_columns.pkl          # Feature names
â”‚
â”œâ”€â”€ ğŸ“ results/                       # Visualizations & reports
â”‚   â”œâ”€â”€ FINAL_COMPREHENSIVE_COMPARISON.png
â”‚   â”œâ”€â”€ performance_improvement_chart.png
â”‚   â”œâ”€â”€ EXECUTIVE_SUMMARY.md
â”‚   â”œâ”€â”€ sensor_correlations.png
â”‚   â”œâ”€â”€ lstm_training_history.png
â”‚   â””â”€â”€ [other visualizations]
â”‚
â”œâ”€â”€ ğŸ“„ download_dataset.py            # Download NASA data from Kaggle
â”œâ”€â”€ ğŸ“„ 01_data_exploration.py         # Explore raw data
â”œâ”€â”€ ğŸ“„ 02_data_preprocessing.py       # Clean & prepare data
â”œâ”€â”€ ğŸ“„ 03_ml_baseline.py              # Train ML models
â”œâ”€â”€ ğŸ“„ 04_deep_learning_lstm.py       # Train LSTM
â”œâ”€â”€ ğŸ“„ 05_cnn_model.py                # Train CNN
â”œâ”€â”€ ğŸ“„ 06_bilstm_model.py             # Train Bi-LSTM
â”œâ”€â”€ ğŸ“„ 07_hybrid_cnn_lstm.py          # Train Hybrid (Best!)
â”œâ”€â”€ ğŸ“„ 08_hyperparameter_tuning.py    # Optimize models
â”œâ”€â”€ ğŸ“„ 09_final_comparison.py         # Compare all models
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python packages needed
â””â”€â”€ ğŸ“„ README.md                      # This file
```

---

## Technical Details

### Metrics Explained Simply

- **RMSE (Root Mean Square Error)**: Average prediction error in cycles
  - Lower is better
  - Our best: 5.27 cycles (like being off by 1 day)
  
- **MAE (Mean Absolute Error)**: Average difference between prediction and reality
  - Lower is better
  - Our best: 4.09 cycles
  
- **RÂ² Score**: How much of the pattern does the model understand?
  - 0 = random guessing
  - 1 = perfect prediction
  - Our best: 0.9915 (99.15% accurate!)

### Training Configuration

```
Deep Learning Models Settings:
â”œâ”€ Sequence Length: 50 cycles (look at last 50 measurements)
â”œâ”€ Batch Size: 64 (process 64 examples at once)
â”œâ”€ Optimizer: Adam (smart learning algorithm)
â”œâ”€ Learning Rate: 0.001 (how fast to learn)
â”œâ”€ Early Stopping: Stop if no improvement for 15 epochs
â””â”€ Learning Rate Decay: Reduce by 50% if stuck for 5 epochs
```

---

## Real-World Impact

### What This Means for Airlines

```
âŒ Without This System:
   â†’ Engine fails unexpectedly
   â†’ Emergency landing required  
   â†’ Cost: $1-5 million
   â†’ Safety risk: HIGH
   â†’ Downtime: 1-2 weeks

âœ… With This System:
   â†’ Predict failure 1 day in advance
   â†’ Schedule maintenance during routine check
   â†’ Cost: $50,000-100,000
   â†’ Safety risk: MINIMAL
   â†’ Downtime: 1-2 days
   
   Savings: 90% cost reduction + Much safer!
```

---

## References

### Dataset
- **Saxena, A., & Goebel, K. (2008)**. Turbofan Engine Degradation Simulation Data Set. NASA Ames Prognostics Data Repository.
- **Link**: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/

### Research Papers
- Saxena, A., Goebel, K., Simon, D., & Eklund, N. (2008). "Damage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation", PHM Conference.

### Technologies Used
- **Deep Learning**: TensorFlow 2.15, Keras
- **Machine Learning**: Scikit-learn, XGBoost, LightGBM
- **Data Processing**: Pandas, NumPy
- **Visualization**: Matplotlib, Seaborn

---

## License

This project is for educational and research purposes.

---